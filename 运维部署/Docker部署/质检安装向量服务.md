### 步骤 2：下载 Python 3.12.2 源码

```
cd /tmp
wget https://www.python.org/ftp/python/3.12.2/Python-3.12.2.tgz
tar -xzf Python-3.12.2.tgz
cd Python-3.12.2
```

---

### 🔧 步骤 3：配置并编译

```
# 配置安装路径（推荐 /usr/local）
./configure --enable-optimizations --prefix=/usr/local

# 编译（-j$(nproc) 加速）
make -j$(nproc)

# 安装（会安装到 /usr/local/bin/python3.12）
sudo make altinstall
```

> ⚠️ 使用 `altinstall` 而不是 `install`，避免覆盖系统默认的 `python3`。

---

### 🔧 步骤 4：验证安装

```
python3.12 --version
```

✅ 应输出：

```
Python 3.12.2
```

同时检查 `pip` 是否可用：

```
python3.12 -m pip --version
```

---

##  测试：创建虚拟环境

```
cd /data/app/zhijian/bge-service
python3.12 -m venv venv
source venv/bin/activate
python --version  # 应显示 3.12.2
```

成功！现在你可以继续部署你的 BGE 服务了。

#### 上传py文件

```python
"""  
BGE 中文嵌入服务 v1.0（增强版）  
功能：将中文文本转为向量，并记录详细日志  
模型：BAAI/bge-m3 或其他  
接口：POST /embed  
"""  
  
from flask import Flask, request, jsonify  
from sentence_transformers import SentenceTransformer  
import torch  
import logging  
import time  
import os  
from datetime import datetime  
  
app = Flask(__name__)  
  
# ======================  
# 配置参数  
# ======================  
MODEL_NAME = "BAAI/bge-m3"  # 可更换为 bge-base-zh-v1.5 等  
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  
PORT = 10000  
  
# ======================  
# 日志配置  
# ======================  
# 创建日志目录  
LOG_DIR = "./logs"  
os.makedirs(LOG_DIR, exist_ok=True)  
  
# 配置日志器  
logger = logging.getLogger("BGEEmbeddingService")  
logger.setLevel(logging.INFO)  
  
# 避免重复添加 handlerif not logger.handlers:  
    # 文件处理器：按天分割日志  
    file_handler = logging.FileHandler(  
        os.path.join(LOG_DIR, f"bge_service_{datetime.now().strftime('%Y%m%d')}.log"),  
        encoding='utf-8'  
    )  
    file_handler.setLevel(logging.INFO)  
  
    # 控制台处理器  
    console_handler = logging.StreamHandler()  
    console_handler.setLevel(logging.INFO)  
  
    # 日志格式  
    formatter = logging.Formatter(  
        '[%(asctime)s] %(levelname)s [%(funcName)s] %(message)s',  
        datefmt='%Y-%m-%d %H:%M:%S'  
    )  
    file_handler.setFormatter(formatter)  
    console_handler.setFormatter(formatter)  
  
    logger.addHandler(file_handler)  
    logger.addHandler(console_handler)  
  
# ======================  
# 加载模型  
# ======================  
print(f"🚀 正在加载模型: {MODEL_NAME}")  
print(f"💻 设备: {DEVICE}")  
model = SentenceTransformer(MODEL_NAME, device=DEVICE)  
print("✅ 模型加载完成！服务已启动")  
  
# 测试维度  
test_emb = model.encode(["测试"])  
print(f"🔍 模型实际输出维度: {len(test_emb[0])}")  
  
  
@app.route('/health', methods=['GET'])  
def health():  
    try:  
        dummy_emb = model.encode(["test"])  
        real_dimensions = len(dummy_emb[0])  
        status = "ok"  
        message = "BGE 中文向量服务运行正常"  
        logger.info("健康检查: 成功")  
    except Exception as e:  
        status = "error"  
        real_dimensions = 0  
        message = f"模型加载异常: {str(e)}"  
        logger.error(f"健康检查失败: {e}")  
  
    return jsonify({  
        "status": status,  
        "model": MODEL_NAME,  
        "device": DEVICE,  
        "dimensions": real_dimensions,  
        "message": message  
    })  
  
  
@app.route('/embed', methods=['POST'])  
def embed():  
    """  
    接收中文文本，返回向量  
    输入: {"input": ["文本1", "文本2"]} 或 {"input": "单个文本"}  
    输出: {"data": [{"embedding": [...]}, ...], "count": 2, "model": "...", ...}  
    """    # 记录请求开始时间  
    start_time = time.time()  
  
    # 获取请求 ID（简单用时间戳）  
    request_id = datetime.now().strftime("%Y%m%d%H%M%S_%f")  
  
    # 记录原始请求  
    raw_data = request.get_data(as_text=True)  
    logger.info(f"[{request_id}] 收到请求: {raw_data}")  
  
    try:  
        data = request.get_json()  
  
        if not data or 'input' not in data:  
            error_msg = "缺少 'input' 字段"  
            logger.warning(f"[{request_id}] 请求参数错误: {error_msg}")  
            return jsonify({"error": error_msg}), 400  
  
        texts = data['input']  
        if isinstance(texts, str):  
            texts = [texts]  
        elif not isinstance(texts, list):  
            error_msg = "'input' 必须是字符串或字符串列表"  
            logger.warning(f"[{request_id}] 请求参数错误: {error_msg}")  
            return jsonify({"error": error_msg}), 400  
  
        if len(texts) == 0:  
            error_msg = "'input' 列表不能为空"  
            logger.warning(f"[{request_id}] 请求参数错误: {error_msg}")  
            return jsonify({"error": error_msg}), 400  
  
        # 生成向量  
        embeddings = model.encode(texts, normalize_embeddings=True)  
        embeddings_list = embeddings.tolist()  
  
        # 构造新格式：data 数组，每个元素是 {"embedding": [...]}        data_list = [{"embedding": emb} for emb in embeddings_list]  
  
        # 计算耗时  
        duration = round((time.time() - start_time) * 1000, 2)  # 毫秒  
  
        response = {  
            "data": data_list,  
            "count": len(data_list),  
            "model": MODEL_NAME,  
            "request_id": request_id,  
            "processing_time_ms": duration  
        }  
  
        # 记录成功响应  
        logger.info(f"[{request_id}] 处理完成 | 耗时: {duration}ms | 文本数: {len(texts)}")  
  
        return jsonify(response)  
  
    except Exception as e:  
        duration = round((time.time() - start_time) * 1000, 2)  
        error_msg = f"向量化失败: {str(e)}"  
        logger.error(f"[{request_id}] 处理异常: {error_msg} | 耗时: {duration}ms")  
        return jsonify({"error": error_msg}), 500  
  
  
if __name__ == '__main__':  
    app.run(host='0.0.0.0', port=PORT, debug=False)
```

## 🚀mg 部署步骤（在 Linux 服务器上操作）

### ✅ 前提检查

```
# 1. 确认 Python 版本
python3.12 --version    # 应输出 Python 3.12.x

# 2. 检查磁盘空间（至少需要 8GB 可用）
df -h /data             # 或 df -h /home

# 3. 确保能访问外网（测试 Hugging Face）
curl -I https://huggingface.co
```

> ⚠️ **BGE-M3 模型约 6GB，依赖包约 2GB，总共需 ≥8GB 可用空间**

---

### 🔧 步骤 1：创建项目目录 & 虚拟环境

```
# 进入你的项目路径
cd /data/app/zhijian/bge-service

# 创建虚拟环境（使用 Python 3.12）
python3.12 -m venv venv

# 激活环境
source venv/bin/activate
```

---

### 🔧 步骤 3：安装依赖（使用清华源加速）

```
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple \
  flask \
  torch \
  sentence-transformers==3.0.1 \
  transformers>=4.38.0
```

> 💡 这会自动安装：
>
> - `torch`（默认 CPU 版，除非你有 GPU）
> - `sentence-transformers`
> - 所有依赖（numpy, tokenizers, safetensors 等）

✅ 安装成功后，验证：

```
python -c "import torch; print('Torch OK')"
python -c "from sentence_transformers import SentenceTransformer; print('SBert OK')"
```

---

### 🔧 步骤 4：放置你的服务代码

确保 `bge_service.py` 已在当前目录：

bash

编辑

```
ls bge_service.py
```

> 如果没有，用 `scp` 从本地上传：
>
> bash
>
> 编辑
>
> ```
> scp bge_service.py user@server:/data/app/zhijian/bge-service/
> ```

---

### 🔧 步骤 5：首次运行（自动下载 BGE-M3 模型）

bash

编辑

```
python bge_service.py
```

> ⏳ **第一次会下载模型（约 6GB），请耐心等待（5~20 分钟，取决于网速）**
>
> 日志会显示：
>
> text
>
> 编辑
>
> ```
> 🚀 正在加载模型: BAAI/bge-m3
> 💻 设备: cpu
> ...
> ✅ 模型加载完成！服务已启动
> 🔍 模型实际输出维度: 1024
> ```

> ✅ 后续再运行就 **秒启动**（模型缓存在 `~/.cache/huggingface/hub/`）

---

### 🔧 步骤 6：后台运行（可选）

使用 `nohup` 或 `screen` 让服务常驻：

bash

编辑

```
nohup python bge_service.py > service.log 2>&1 &
```

查看日志：

bash

编辑

```
tail -f service.log
```

---

### 🔍 验证服务是否正常

bash

编辑

```
# 健康检查
curl http://localhost:9099/health

# 测试 embedding
curl -X POST http://localhost:9099/embed \
  -H "Content-Type: application/json" \
  -d '{"input": ["你好，世界！"]}'
```

应返回类似：

json

编辑

```
{
  "data": [{"embedding": [0.123, -0.456, ...]}],
  "count": 1,
  "model": "BAAI/bge-m3",
  ...
}
```

---

## 📌 注意事项

| 问题         | 解决方案                                                                                                        |
|------------|-------------------------------------------------------------------------------------------------------------|
| **磁盘空间不足** | 清理 `/tmp`、`~/.cache/pip`，或扩容                                                                                |
| **下载模型超时** | 设置 HF 镜像（见下方）                                                                                               |
| **想用 GPU** | 先装 CUDA 驱动，然后 `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121` |
| **防火墙限制**  | 联系运维开放 `huggingface.co:443`                                                                                 |

---

### 💡 （可选）设置 Hugging Face 镜像（国内加速）

如果你在国内，可设置镜像加速模型下载：

bash

编辑

```
# 临时生效
HF_ENDPOINT=https://hf-mirror.com python bge_service.py
```

或永久设置：

bash

编辑

```
echo 'export HF_ENDPOINT=https://hf-mirror.com' >> ~/.bashrc
source ~/.bashrc
```

> 🔗 镜像站：https://hf-mirror.com

---

## ✅ 总结：你现在只需在服务器执行

bash

编辑

```
cd /data/app/zhijian/bge-service
python3.12 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple flask torch sentence-transformers==3.0.1 transformers>=4.38.0
# 确保 bge_service.py 在当前目录
python bge_service.py
```

> ✅ 完成！服务将在 `http://0.0.0.0:9099` 监听